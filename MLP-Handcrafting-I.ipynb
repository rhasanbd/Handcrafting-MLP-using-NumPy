{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron - Single Hidden Layer \n",
    "\n",
    "You will create a single hidden-layer MLP for performing **Binary Classification** on a synthetic dataset. The backpropagation algorithm should be implemented using the batch Gradient Descent. You will use the mean-squared error as the loss function.\n",
    "\n",
    "- You will use only the NumPy library for creating the MLP model.\n",
    "\n",
    "\n",
    "## MLP Architecture\n",
    "\n",
    "You will create a MLP with 3 layers.\n",
    "- First layer: input layer (it should have two \"neurons\" since the input data is 2D)\n",
    "- Second layer: hidden layer (it should have 4 neurons)\n",
    "- Third layer: output/classification layer (it should have only one neuron since it's a binary clasification problem)\n",
    "\n",
    "### Bias weights\n",
    "Neurons in the hidden layer and the final layer will have a bias weight. For example:\n",
    "- Hidden layer: the 4 neurons in the hidden layer will have 4 bias weights. \n",
    "- Final layer: the single neuron in the final layer will have a bias weight. \n",
    "\n",
    "These bias weights are added as a separate row in the weight matrices for the hidden layer and the final layer. Consequently, both in the input and the hidden, a bias neuron is added with the feature neurons. This is done by adding a column of 1s with the input data and the hidden layer activation signal data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.datasets import make_circles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Linearly Non-Separable Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic linearly non-separable dataset\n",
    "X, y = make_circles(500, factor=0.4, noise=0.15)\n",
    "\n",
    "print(\"X dimension: \", X.shape)\n",
    "print(\"y dimension: \", y.shape)\n",
    "print(\"Number of classes: \", len(np.unique(y)))\n",
    "print(\"Class labels: \", np.unique(y))\n",
    "\n",
    "\n",
    "# The label variable is required by the matplotlib \"scatter\" function\n",
    "label = y\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Here y is a 1D array (m elements)\n",
    "Reshape it to create a 2D array (m x 1)\n",
    "'''  \n",
    "y = y.reshape(len(y), 1)\n",
    "print(\"\\ny dimension (after reshaping): \", y.shape)\n",
    "\n",
    "\n",
    "# Display the data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=label, s=50, cmap='autumn')\n",
    "plt.title(\"Data Distribution\", fontsize=16)\n",
    "plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "plt.ylabel(\"$x_2$\", fontsize=14)\n",
    "plt.axis([-1.5, 1.5, -1.7, 1.7])\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Train & Test Subsets, Then Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "print(\"X train dimension: \", X_train.shape)\n",
    "print(\"X test dimension: \", X_test.shape)\n",
    "\n",
    "print(\"\\ny_train dimension: \", y_train.shape)\n",
    "print(\"y_test dimension: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logistic sigmoid function\n",
    "def sigmoid(z):\n",
    "    TBD\n",
    "\n",
    "# Define a function to compute the derivative of sigmoid\n",
    "# Formula: derivative of sigmoid for input z = sigmoid(z) * (1 - sigmoid(z))\n",
    "def derivativeSigmoid(z):\n",
    "    TBD\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the weights of a 2D weight matrix with uniform random values\n",
    "# You may use the numpy.random.randn function\n",
    "# https://numpy.org/doc/stable/reference/random/generated/numpy.random.randn.html\n",
    "# It will return a sample (or samples) from the “standard normal” distribution\n",
    "def randWeights(input_neurons, output_neurons):\n",
    "\n",
    "    TBD \n",
    "    \n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation Algorithm\n",
    "\n",
    "Implement the backpropagation algorithm by using batch Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_itr = 2500                    # Max no. of iterations for the batch GD algorithm\n",
    "eta = 0.9                         # Learning rate\n",
    "input_layer_neurons = 2           # No. of columns in \"X_train\" that represent the features (excluding the bias)\n",
    "hidden_layer_neurons = 4          # No. of features (units or neurons) in the hidden layer\n",
    "output_layer_neurons = 1          # No. of output layer units or neurons\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "- Create the weight matrix W_1 for the input-to-hidden layer (hidden layer bias should be included)\n",
    "- initialize with uniform random values by using the randWeights() function\n",
    "Dimension of W_1: No. of input layer neurons + bias x No. of hidden layer neurons (excluding bias) \n",
    "'''\n",
    "W_1 = TBD  \n",
    "print(\"Initial W_1: \\n\", W_1)\n",
    "\n",
    "\n",
    "'''\n",
    "- Create the weight matrix W_2 for hidden-to-output layer (final layer bias should be included)\n",
    "- initialize with uniform random values by using the randWeights() function\n",
    "Dimension of W_2: No. of hidden layer neurons + bias x No. of output layer neurons  \n",
    "'''\n",
    "W_2 = TBD  \n",
    "print(\"Initial W_2: \\n\", W_2)\n",
    "\n",
    "\n",
    "# Create a list to store the traiing loss values at each iteration\n",
    "loss_training = []\n",
    "\n",
    "\n",
    "'''\n",
    "Backpropagation\n",
    "'''\n",
    "for i in range(max_itr):\n",
    "        \n",
    "    #------------------------------Forward propagation------------------------------ \n",
    "    \n",
    "\n",
    "    \n",
    "    # Add bias (a column of all 1s) to the training data matrix X_train \n",
    "    # - Denote it by \"a1\"\n",
    "    a1 = \n",
    "    \n",
    " \n",
    "    # Compute the linear (affine) combination of the input (weighted sum)\n",
    "    # It will produce the input signal z2 for the hidden layer\n",
    "    # Dimension of z2: (No. of rows in \"X_train\" x No. of hidden_layer_neurons)\n",
    "    z2 =  \n",
    "\n",
    "    \n",
    "    # Compute the activation output a2 of the hidden layer neurons\n",
    "    # Dimension of a2: (No. of rows in \"X_train\" x No. of hidden_layer_neurons)\n",
    "    a2 = \n",
    "    \n",
    "    # Add a bias neuron for the hidden layer\n",
    "    # Dimension of a2 with bias: (No. of rows in \"X_train\" x No. of hidden_layer_neurons + bias)\n",
    "    a2 =  \n",
    "\n",
    "\n",
    "    # Compute the linear (affine) combination of the hidden layer activation output a2\n",
    "    # It will produce the input signal z3 for the final layer\n",
    "    # Dimension of z3: (No. of rows in \"X_train\" x No. of output layer neurons)\n",
    "    z3 =   \n",
    "    \n",
    "    # Compute the activation output vector a3 of the output layer neuron\n",
    "    # Dimension of a3: (No. of rows in \"X_train\" x No. of output layer neurons)\n",
    "    a3 =  \n",
    "    \n",
    "    \n",
    " \n",
    "    # Compute the unregularized loss using MSE\n",
    "    L = \n",
    "    \n",
    "    # Store the training loss in a list\n",
    "    loss_training.append(L)\n",
    "\n",
    "   \n",
    "\n",
    "    #------------------------------Backward propagation------------------------------\n",
    "     \n",
    "    '''\n",
    "    For updating the weight matrices W_1 and W_2, following formulas are used.\n",
    "        W_2 = W_2 - eta/m * grad(L)/grad(W_2) \n",
    "        W_1 = W_1 - eta/m * grad(L)/grad(W_1) \n",
    "    \n",
    "    \n",
    "    The two loss gradients in the above equations are computed as follows.\n",
    "        grad(L)/grad(W_2) = delta_3 (matrix multiplication) a2\n",
    "        grad(L)/grad(W_1) = delta_2 (matrix multiplication) a1\n",
    "        \n",
    "    In the above two equations:\n",
    "        delta_3: error due to the input to the final layer\n",
    "        delta_2: error due to the input to the hidden layer\n",
    "        \n",
    "     Formulas for delta_3 & delta_2 are:\n",
    "        delta_3 = (a3 - y) * activation_derivative_z3\n",
    "        delta_2 = (delta_3 (matrix multiplication) W_2) * activation_derivative_z2\n",
    "        \n",
    "        Here:\n",
    "        - Remove the bias row from W_2. Because we don't compute the bias-neuron errors.\n",
    "          They are not connected to the previous layer neurons, hence don't participate in the \n",
    "          error back propagation.\n",
    "        \n",
    "        - * refers to elementwise multiplication\n",
    "    '''\n",
    " \n",
    "\n",
    "\n",
    "    # Compute delta_3\n",
    "    # Dimension of delta_3: (No. of rows in \"X_train\" x No. of output layer neurons)\n",
    "    delta_3 =  \n",
    "\n",
    "    \n",
    "    '''\n",
    "    Compute: grad(L)/grad(W_2) = delta_3 (matrix multiplication) a2\n",
    "    Dimension of delta_3: (#rows in \"X_train\" x #output layer neurons) \n",
    "    Dimension of a2: (#rows in \"X_train\" x #hidden layer neurons+bias) \n",
    "    The matrix multiplication should create \"grad_L_for_W_2\" that has the dimension of W_2\n",
    "     -- #hidden layer neurons + bias x #output layer neurons\n",
    "    Therefore, to match the dimension of \"W_2\", create transpose of a2, \n",
    "                                      then compute matrix multiplication with \"delta_3\" \n",
    "    '''\n",
    "    grad_L_for_W_2 = \n",
    "\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Compute delta_2 using the formula:\n",
    "        delta_2 = (delta_3 (matrix multiplication) W_2) * activation_derivative_z2\n",
    "    \n",
    "    Dimension of delta_2 should be: (#rows in \"X_train\" x #hidden layer neurons excluding bias)\n",
    "    \n",
    "    - First, remove the bias row from W_2\n",
    "    - Then, perform matrix multiplication between delta_3 & W_2.\n",
    "      To match dimension, W_2 (no bias) needs to be transposed.\n",
    "    - Finally, perform elementwise multiplication with the activation_derivative_z2\n",
    "    '''    \n",
    "    delta_2 = \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Compute: grad(L)/grad(W_1) = delta_2 (matrix multiplication) a1\n",
    "    Dimension of delta_2: (#rows in \"X_train\" x #hidden layer neurons excluding bias) \n",
    "    Dimension of a1: (#rows in \"X_train\" x #input layer neurons + bias) \n",
    "    The matrix multiplication should create \"grad_L_for_W_1\" that has the dimension of W_1\n",
    "    -- #input layer neurons + bias x #hidden layer neurons excluding bias\n",
    "    Therefore, to match dimension of \"W_1\", transpose a1 , \n",
    "                                      then perform matrix multiplication with \"delta_2\" \n",
    "    '''\n",
    "    grad_L_for_W_1 = \n",
    " \n",
    " \n",
    "    # Update W_2:\n",
    "    # Formula: W_2 = W_2 - (eta/m) * grad(L)/grad(W_2)\n",
    "    W_2 = \n",
    "    \n",
    "    # Update W_1:\n",
    "    # Formula: W_1 = W_1 - (eta/m) * grad(L)/grad(W_1)\n",
    "    W_1 = \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Write a function for predicting the output probability\n",
    "# It is simply the forward propagation part from above\n",
    "# It returns the probability to belong to the positive class\n",
    "def predict_proba(X, W_1, W_2):\n",
    "    \n",
    "    # Add bias (a column of all 1s) to the input data matrix X \n",
    "    # - Denote it by \"a1\"\n",
    "    a1 = \n",
    "    \n",
    " \n",
    "    # Compute the linear (affine) combination of the input (weighted sum)\n",
    "    # It will produce the input signal z2 for the hidden layer\n",
    "    # Dimension of z2: (No. of rows in \"X\" x No. of hidden_layer_neurons)\n",
    "    z2 =  \n",
    "\n",
    "    \n",
    "    # Compute the activation output a2 of the hidden layer neurons\n",
    "    # Dimension of a2: (No. of rows in \"X\" x No. of hidden_layer_neurons)\n",
    "    a2 = \n",
    "    \n",
    "    # Add a bias neuron for the hidden layer\n",
    "    # Dimension of a2 with bias: (No. of rows in \"X\" x No. of hidden_layer_neurons + bias)\n",
    "    a2 =  \n",
    "\n",
    "\n",
    "    # Compute the linear (affine) combination of the hidden layer activation output a2\n",
    "    # It will produce the input signal z3 for the final layer\n",
    "    # Dimension of z3: (No. of rows in \"X\" x No. of output layer neurons)\n",
    "    z3 =   \n",
    "    \n",
    "    # Compute the activation output vector a3 of the output layer neuron\n",
    "    # Dimension of a3: (No. of rows in \"X\" x No. of output layer neurons)\n",
    "    a3 =  \n",
    "    \n",
    "    \n",
    "    return a3\n",
    "\n",
    "\n",
    "\n",
    "# Write a function for predicting the labels based on the predictd probabilities\n",
    "# Note the the final layer output a3 is the predicted probability\n",
    "def predict_labels(y, y_pred_proba):\n",
    "    \n",
    "    # Get the number of samples\n",
    "    m = len(y)\n",
    "    \n",
    "    # Create an empty m x 1 array to store the predicted labels\n",
    "    y_predicted = \n",
    "\n",
    "    # Compute the predicted labels by comparing the positive class (label 1) probabilities with threshold 0.5\n",
    "    TBD\n",
    "    \n",
    "    return y_predicted\n",
    "\n",
    "\n",
    "\n",
    "# Plot the training loss against iterations\n",
    "plt.figure(figsize=(12, 6))  \n",
    "plt.plot(loss_training,  \"r--\", alpha=1.0, linewidth=3.0, label=\"Training Loss\")\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.legend(loc=\"best\", fontsize=14) \n",
    "plt.title(\"Training Loss vs Iterations\")\n",
    "plt.show()\n",
    "\n",
    "# Display the final weights\n",
    "print(\"Final W_1: \\n\", W_1)\n",
    "print(\"\\nFinal W_2: \\n\", W_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the positive class probabilities for the test data\n",
    "y_predicted_proba_test = predict_proba(X_test, W_1, W_2)\n",
    "\n",
    "# Predict the labels for the test data       \n",
    "y_predicted_test = predict_labels(y_test, y_predicted_proba_test)\n",
    "\n",
    "\n",
    "# Accuracy of training data\n",
    "print(\"\\nNo. of correct test predictions: %d/%d\" % (np.sum(y_predicted_test == y_test), len(y_test)))\n",
    "\n",
    "accuracy_test = np.mean(y_predicted_test == y_test)\n",
    "print(\"\\nTest Accuracy: \", accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Plotting Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_boundary_class_colored_mlp(prediction, W_1, W_2, X, plotDistanceFromHyperplane=False, colorBar=False):\n",
    "    \n",
    "    # Get the min and max value of feature x1\n",
    "    x1min, x1max = X[:,0].min() - 1, X[:, 0].max() + 1\n",
    "    \n",
    "    # Get the min and max value of feature x2\n",
    "    x2min, x2max = X[:,1].min() - 1, X[:, 1].max() + 1\n",
    "    \n",
    "    # Create the mesh grid\n",
    "    x1s = np.linspace(x1min, x1max, 100)\n",
    "    x2s = np.linspace(x2min, x2max, 100)\n",
    "    x1, x2 = np.meshgrid(x1s, x2s)\n",
    "    \n",
    "    \n",
    "    # Create pairs of new points from the grid\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    \n",
    "    \n",
    "    # Compute the class predictions for all new points\n",
    "    #y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "    y_pred = prediction(X_new, W_1, W_2).reshape(x1.shape)\n",
    "    \n",
    "    \n",
    "    # Generate the contourf plot for the predictions\n",
    "    #plt.contourf(x1, x2, y_pred, cmap=plt.cm.RdBu, alpha=0.6)\n",
    "    plt.contourf(x1, x2, y_pred, cmap=plt.cm.summer, alpha=0.9)\n",
    "    \n",
    "    \n",
    "    if(plotDistanceFromHyperplane == True):\n",
    "    \n",
    "        # Compute the signed distance of a sample to the hyperplane for all new points\n",
    "        y_decision = clf.decision_function(X_new).reshape(x1.shape)\n",
    "\n",
    "        # Generate the contourf plot for the distance of all points from the hyperplane\n",
    "        plt.contourf(x1, x2, y_decision, cmap=plt.cm.seismic, alpha=0.2)\n",
    "    \n",
    "    if(colorBar==True):\n",
    "        plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Decision Boundary for the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "decision_boundary_class_colored_mlp(predict_proba, W_1, W_2, X_test)\n",
    "plt.plot(X_test[y_test.ravel()==0, 0], X_test[y_test.ravel()==0, 1], \"ko\", markersize=5)\n",
    "plt.plot(X_test[y_test.ravel()==1, 0], X_test[y_test.ravel()==1, 1], \"ro\", markersize=5)\n",
    "plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "plt.ylabel(\"$x_2$\", fontsize=14)\n",
    "\n",
    "plt.title(\"Decision Boundary for Test Data:\\n Test Accuracy: %f\"% \n",
    "         (accuracy_test) , fontsize=16)\n",
    "\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
