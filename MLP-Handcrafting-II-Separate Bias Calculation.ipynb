{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron - Single Hidden Layer (bias weights caculated separately)\n",
    "\n",
    "You will create a single hidden-layer MLP for performing **Binary Classification** on a synthetic dataset. The backpropagation algorithm should be implemented using the batch Gradient Descent. You will use the mean-squared error as the loss function.\n",
    "\n",
    "- You will use only the NumPy library for creating the MLP model.\n",
    "\n",
    "\n",
    "\n",
    "## MLP Architecture\n",
    "\n",
    "You will create a MLP with 3 layers.\n",
    "- First layer: input layer (it should have two \"neurons\" since the input data is 2D)\n",
    "- Second layer: hidden layer (it should have 4 neurons)\n",
    "- Third layer: output/classification layer (it should have only one neuron since it's a binary clasification problem)\n",
    "\n",
    "### Bias weights\n",
    "The bias weights for the hidden layer and the final layer neurons are **computed separately**.\n",
    "- Hidden layer: each of the 4 neurons in the hidden layer will have a bias weight. Thus, the hidden layer bias weight should be a (4 x 1) array\n",
    "- Final layer: the single neuron in the final layer will have a bias weight. Thus, the final layer bias weight should be a (1 x 1) array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.datasets import make_circles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Linearly Non-Separable Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic linearly non-separable dataset\n",
    "X, y = make_circles(500, factor=0.4, noise=0.15)\n",
    "\n",
    "print(\"X dimension: \", X.shape)\n",
    "print(\"y dimension: \", y.shape)\n",
    "print(\"Number of classes: \", len(np.unique(y)))\n",
    "print(\"Class labels: \", np.unique(y))\n",
    "\n",
    "\n",
    "# The label variable is required by the matplotlib \"scatter\" function\n",
    "label = y\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Here y is a 1D array (m elements)\n",
    "Reshape it to create a 2D array (m x 1)\n",
    "'''  \n",
    "y = y.reshape(len(y), 1)\n",
    "print(\"\\ny dimension (after reshaping): \", y.shape)\n",
    "\n",
    "\n",
    "# Display the data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=label, s=50, cmap='autumn')\n",
    "plt.title(\"Data Distribution\", fontsize=16)\n",
    "plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "plt.ylabel(\"$x_2$\", fontsize=14)\n",
    "plt.axis([-1.5, 1.5, -1.7, 1.7])\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Train & Test Subsets, Then Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "print(\"X train dimension: \", X_train.shape)\n",
    "print(\"X test dimension: \", X_test.shape)\n",
    "\n",
    "print(\"\\ny_train dimension: \", y_train.shape)\n",
    "print(\"y_test dimension: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logistic sigmoid function\n",
    "def sigmoid(z):\n",
    "    TBD\n",
    "\n",
    "# Define a function to compute the derivative of sigmoid\n",
    "# Formula: derivative of sigmoid for input z = sigmoid(z) * (1 - sigmoid(z))\n",
    "def derivativeSigmoid(z):\n",
    "    TBD\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the weights of a 2D weight matrix with uniform random values\n",
    "# You may use the numpy.random.randn function\n",
    "# https://numpy.org/doc/stable/reference/random/generated/numpy.random.randn.html\n",
    "# It will return a sample (or samples) from the “standard normal” distribution\n",
    "def randWeights(input_neurons, output_neurons):\n",
    "\n",
    "    TBD \n",
    "    \n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation Algorithm\n",
    "\n",
    "Implement the backpropagation algorithm by using batch Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the training dataset\n",
    "m = X_train.shape[0]\n",
    "\n",
    "max_itr = 2000                    # Max no. of iterations for the batch GD algorithm\n",
    "eta = 0.9                         # Learning rate\n",
    "input_layer_neurons = 2           # No. of columns in \"X_train\" that represent the features (excluding the bias)\n",
    "hidden_layer_neurons = 4          # No. of features (units or neurons) in the hidden layer\n",
    "output_layer_neurons = 1          # No. of output layer units or neurons\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "- Create the weight matrix W_1 for the input-to-hidden layer\n",
    "- initialize with uniform random values by using the randWeights() function\n",
    "Dimension of W_1: No. of input layer neurons x No. of hidden layer neurons  \n",
    "'''\n",
    "W_1 =   \n",
    "print(\"Initial W_1: \\n\", W_1)\n",
    "\n",
    "\n",
    "'''\n",
    "- Create the weight matrix W_2 for hidden-to-output layer\n",
    "- initialize with uniform random values by using the randWeights() function\n",
    "Dimension of W_2: No. of hidden layer neurons x No. of output layer neurons  \n",
    "'''\n",
    "W_2 =  \n",
    "print(\"Initial W_2: \\n\", W_2)\n",
    "\n",
    "'''\n",
    "- Create the bias weights for the hidden layer (b_1) and the final layer (b_2)\n",
    "- initialize with zeros\n",
    "'''\n",
    "b_1 = \n",
    "b_2 = \n",
    "\n",
    "\n",
    "# Create a list to store the traiing loss values at each iteration\n",
    "loss_training = []\n",
    "\n",
    "\n",
    "'''\n",
    "Backpropagation\n",
    "'''\n",
    "for i in range(max_itr):\n",
    "        \n",
    "    #------------------------------Forward propagation------------------------------ \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Denote input feature matrix with a1\n",
    "    a1 = \n",
    "    \n",
    "    # Create a dummy feature ndarray with value 1 for the input data\n",
    "    a1_0 = \n",
    "    \n",
    "    \n",
    " \n",
    "    # Compute the affine combination of the input\n",
    "    # - this is equal to the weighted sum + bias\n",
    "    # It will produce the input signal z2 for the hidden layer\n",
    "    # Dimension of z2: (No. of rows in \"X_train\" x No. of hidden_layer_neurons)\n",
    "    z2 = \n",
    "\n",
    "    \n",
    "    # Compute the activation output a2 of the hidden layer neurons\n",
    "    # Dimension of a2: (No. of rows in \"X_train\" x No. of hidden_layer_neurons)\n",
    "    a2 = \n",
    "    \n",
    "    \n",
    "    # Create a dummy feature ndarray with value 1 for the hidden layer data\n",
    "    a2_0 = \n",
    "    \n",
    "   \n",
    "    # Compute the affine combination of the hidden layer activation output a2\n",
    "    # - this is equal to the weighted sum + bias\n",
    "    # It will produce the input signal z3 for the final layer\n",
    "    # Dimension of z3: (No. of rows in \"X_train\" x No. of output layer neurons)\n",
    "    z3 = \n",
    "    \n",
    "    # Compute the activation output vector a3 of the output layer neuron\n",
    "    # Dimension of a3: (No. of rows in \"X_train\" x No. of output layer neurons)\n",
    "    a3 =   \n",
    "    \n",
    "    \n",
    "    # Compute the unregularized loss using MSE\n",
    "    L = \n",
    "    \n",
    "    # Store the training loss in a list\n",
    "    loss_training.append(L)\n",
    "\n",
    "   \n",
    "\n",
    "    #------------------------------Backward propagation------------------------------\n",
    "     \n",
    "    '''\n",
    "    For updating the weight matrices W_1 and W_2, following formulas are used.\n",
    "        W_2 = W_2 - eta/m * grad(L)/grad(W_2) \n",
    "        W_1 = W_1 - eta/m * grad(L)/grad(W_1) \n",
    "    \n",
    "    \n",
    "    The two loss gradients in the above equations are computed as follows.\n",
    "        grad(L)/grad(W_2) = delta_3 (matrix multiplication) a2\n",
    "        grad(L)/grad(W_1) = delta_2 (matrix multiplication) a1\n",
    "        \n",
    "    In the above two equations:\n",
    "        delta_3: error due to the input to the final layer\n",
    "        delta_2: error due to the input to the hidden layer\n",
    "        \n",
    "    Formulas for delta_3 & delta_2 are:\n",
    "        delta_3 = (a3 - y) * activation_derivative_z3\n",
    "        delta_2 = (delta_3 (matrix multiplication) W_2) * activation_derivative_z2\n",
    "        \n",
    "        Here * refers to elementwise multiplication\n",
    "    '''\n",
    " \n",
    "\n",
    "\n",
    "    # Compute delta_3\n",
    "    # Dimension of delta_3: (No. of rows in \"X_train\" x No. of output layer neurons)\n",
    "    delta_3 = \n",
    "\n",
    "    \n",
    "    '''\n",
    "    Compute los gradient for the final layer weights and bias: \n",
    "    - grad(L)/grad(W_2) = delta_3 (matrix multiplication) a2\n",
    "    - grad(L)/grad(b_2) = delta_3 (matrix multiplication) a2_0\n",
    "    Dimension of delta_3: (#rows in \"X_train\" x #output layer neurons) \n",
    "    Dimension of a2: (#rows in \"X_train\" x #hidden layer neurons+bias) \n",
    "    The matrix multiplication should create \"grad_L_for_W_2\" that has the dimension of W_2\n",
    "     -- #hidden layer neurons + bias x #output layer neurons\n",
    "    Therefore, to match the dimension of \"W_2\", create transpose of a2, \n",
    "                                      then perform matrix multiplication with \"delta_3\" \n",
    "    Perform a similar calculation for grad(L)/grad(b_2)\n",
    "    '''\n",
    "    grad_L_for_W_2 = \n",
    "    \n",
    "    grad_L_for_b_2 = \n",
    "\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Compute delta_2 using the folrmula:\n",
    "        delta_2 = (delta_3 (matrix multiplication) W_2) * activation_derivative_z2\n",
    "    Dimension of delta_2: (#rows in \"X_train\" x #hidden layer neurons+bias)\n",
    "    - First, perform matrix multiplication between delta_3 & W_2. To match dimension, W_2 needs to be transposed.\n",
    "    - Then, take an elementwise product of the result of the above step and activation_derivative_z2\n",
    "    '''\n",
    "    delta_2 = \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Compute los gradient for the hidden layer weights and bias: \n",
    "    - grad(L)/grad(W_1) = delta_2 (matrix multiplication) a1\n",
    "    - grad(L)/grad(b_1) = delta_3 (matrix multiplication) a2_0\n",
    "    \n",
    "    Dimension of delta_2: (#rows in \"X_train\" x #hidden layer neurons+bias) \n",
    "    Dimension of a1: (#rows in \"X_train\" x #input layer neurons+bias) \n",
    "    The matrix multiplication should create \"grad_L_for_W_1\" that has the dimension of W_1\n",
    "    -- #input layer neurons + bias x #hidden layer neurons\n",
    "    Therefore, to match dimension of \"W_1\", create transpose of a1 , \n",
    "                                      then perform matrix multiplication with \"delta_2\" \n",
    "    Perform a similar calculation for grad(L)/grad(b_1)\n",
    "    '''\n",
    "    grad_L_for_W_1 = \n",
    "    grad_L_for_b_1 = \n",
    "    \n",
    " \n",
    " \n",
    "    # Update W_2:\n",
    "    # Formula: W_2 = W_2 - (eta/m) * grad(L)/grad(W_2)\n",
    "    W_2 =  \n",
    "    \n",
    "    \n",
    "    # Update W_1:\n",
    "    # Formula: W_1 = W_1 - (eta/m) * grad(L)/grad(W_1)\n",
    "    W_1 = \n",
    "\n",
    "    # Update the bises using the same formula\n",
    "    b_1 = \n",
    "    b_2 = \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Write a function for predicting the output probability\n",
    "# It is simply the forward propagation part from above\n",
    "# It returns the probability to belong to the positive class\n",
    "def predict_proba(X, W_1, W_2, b_1, b_2):\n",
    "   \n",
    "    # Denote input feature matrix with a1\n",
    "    a1 = \n",
    "    \n",
    "    # Create a dummy feature ndarray with 1s for the input data\n",
    "    a1_0 = \n",
    "    \n",
    "    \n",
    "    # Compute the affine combination of the input\n",
    "    # - this is equal to the weighted sum + bias\n",
    "    # It will produce the input signal z2 for the hidden layer\n",
    "    # Dimension of z2: (No. of rows in \"X_train\" x No. of hidden_layer_neurons)\n",
    "    z2 = \n",
    "\n",
    "    \n",
    "    # Compute the activation output a2 of the hidden layer neurons\n",
    "    # Dimension of a2: (No. of rows in \"X_train\" x No. of hidden_layer_neurons)\n",
    "    a2 = \n",
    "    \n",
    "    \n",
    "    # Create a dummy feature ndarray with 1s for the hidden layer data\n",
    "    a2_0 = \n",
    "    \n",
    "   \n",
    "    # Compute the affine combination of the hidden layer activation output a2\n",
    "    # - this is equal to the weighted sum + bias\n",
    "    # It will produce the input signal z3 for the final layer\n",
    "    # Dimension of z3: (No. of rows in \"X_train\" x No. of output layer neurons)\n",
    "    z3 = \n",
    "    \n",
    "    # Compute the activation output vector a3 of the output layer neuron\n",
    "    # Dimension of a3: (No. of rows in \"X_train\" x No. of output layer neurons)\n",
    "    a3 = \n",
    "    \n",
    "    return a3\n",
    "\n",
    "\n",
    "\n",
    "# Write a function for predicting the labels based on the predictd probabilities\n",
    "def predict_labels(y, y_pred_proba):\n",
    "    \n",
    "    m = len(y)\n",
    "    \n",
    "    # Create an empty m x 1 array to store the predicted labels\n",
    "    y_predicted = \n",
    "\n",
    "    # Compute the predicted labels by comparing the positive class (label 1) probabilities with threshold 0.5\n",
    "    TBD\n",
    "    \n",
    "    return y_predicted\n",
    "\n",
    "\n",
    "\n",
    "# Plot the training loss against iterations\n",
    "plt.figure(figsize=(12, 6))  \n",
    "plt.plot(loss_training,  \"r--\", alpha=1.0, linewidth=3.0, label=\"Training Loss\")\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.legend(loc=\"best\", fontsize=14) \n",
    "plt.title(\"Training Loss vs Iterations\")\n",
    "plt.show()\n",
    "\n",
    "# Display the final weights\n",
    "print(\"Final W_1: \\n\", W_1)\n",
    "print(\"\\nFinal W_2: \\n\", W_2)\n",
    "\n",
    "print(\"Final b_1: \\n\", b_1)\n",
    "print(\"\\nFinal b_2: \\n\", b_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the positive class probabilities for the test data\n",
    "y_predicted_proba_test = predict_proba(X_test, W_1, W_2, b_1, b_2)\n",
    "\n",
    "# Predict the labels for the test data       \n",
    "y_predicted_test = predict_labels(y_test, y_predicted_proba_test)\n",
    "\n",
    "\n",
    "# Accuracy of training data\n",
    "print(\"\\nNo. of correct test predictions: %d/%d\" % (np.sum(y_predicted_test == y_test), len(y_test)))\n",
    "\n",
    "accuracy_test = np.mean(y_predicted_test == y_test)\n",
    "print(\"\\nTest Accuracy: \", accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Plotting Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_boundary_class_colored_mlp_separate_bias(prediction, W_1, W_2, b_1, b_2, \n",
    "                                        X, plotDistanceFromHyperplane=False, colorBar=False):\n",
    "    \n",
    "    # Get the min and max value of feature x1\n",
    "    x1min, x1max = X[:,0].min() - 1, X[:, 0].max() + 1\n",
    "    \n",
    "    # Get the min and max value of feature x2\n",
    "    x2min, x2max = X[:,1].min() - 1, X[:, 1].max() + 1\n",
    "    \n",
    "    # Create the mesh grid\n",
    "    x1s = np.linspace(x1min, x1max, 100)\n",
    "    x2s = np.linspace(x2min, x2max, 100)\n",
    "    x1, x2 = np.meshgrid(x1s, x2s)\n",
    "    \n",
    "    \n",
    "    # Create pairs of new points from the grid\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    \n",
    "    \n",
    "    # Compute the class predictions for all new points\n",
    "    #y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "    y_pred = prediction(X_new, W_1, W_2, b_1, b_2).reshape(x1.shape)\n",
    "    \n",
    "    \n",
    "    # Generate the contourf plot for the predictions\n",
    "    #plt.contourf(x1, x2, y_pred, cmap=plt.cm.RdBu, alpha=0.6)\n",
    "    plt.contourf(x1, x2, y_pred, cmap=plt.cm.summer, alpha=0.9)\n",
    "    \n",
    "    \n",
    "    if(plotDistanceFromHyperplane == True):\n",
    "    \n",
    "        # Compute the signed distance of a sample to the hyperplane for all new points\n",
    "        y_decision = clf.decision_function(X_new).reshape(x1.shape)\n",
    "\n",
    "        # Generate the contourf plot for the distance of all points from the hyperplane\n",
    "        plt.contourf(x1, x2, y_decision, cmap=plt.cm.seismic, alpha=0.2)\n",
    "    \n",
    "    if(colorBar==True):\n",
    "        plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Decision Boundary for the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "decision_boundary_class_colored_mlp_separate_bias(predict_proba, W_1, W_2, X_test)\n",
    "plt.plot(X_test[y_test.ravel()==0, 0], X_test[y_test.ravel()==0, 1], \"ko\", markersize=5)\n",
    "plt.plot(X_test[y_test.ravel()==1, 0], X_test[y_test.ravel()==1, 1], \"ro\", markersize=5)\n",
    "plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "plt.ylabel(\"$x_2$\", fontsize=14)\n",
    "\n",
    "plt.title(\"Decision Boundary for Test Data:\\n Test Accuracy: %f\"% \n",
    "         (accuracy_test) , fontsize=16)\n",
    "\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
